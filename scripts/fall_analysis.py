import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd
from datetime import datetime
import argparse
from collections import defaultdict
import matplotlib.animation as animation
from typing import List, Dict, Any, Tuple

class FallAnalyzer:
    def __init__(self, data_dir: str = "data", predictions_dir: str = "predictions", model_path: str = None):
        """Initialize the analyzer with correct paths relative to script location."""
        # Get the script's directory
        script_dir = Path(__file__).parent.parent
        self.data_dir = script_dir / data_dir
        self.predictions_dir = script_dir / predictions_dir
        self.model_path = model_path
        self.recorded_sequences = []
        self.prediction_sequences = []
        self.grid_width = 12
        self.grid_height = 15
        
    def load_data(self, recorded_file: str = None, prediction_file: str = None):
        """Load recorded and prediction data."""
        if recorded_file:
            with open(self.data_dir / recorded_file) as f:
                self.recorded_sequences.append(json.load(f))
        else:
            for file in self.data_dir.glob("recorded_sequences_*.json"):
                with open(file) as f:
                    self.recorded_sequences.append(json.load(f))
                    
        if prediction_file:
            with open(self.predictions_dir / prediction_file) as f:
                data = json.load(f)
                stored_model = data["metadata"].get("model_path", "")
                if self.model_path is None or stored_model == self.model_path:
                    self.prediction_sequences.append(data)
                    print(f"Loaded predictions from: {prediction_file}")
                else:
                    print(f"Skipping {prediction_file} - generated by different model: {stored_model}")
        else:
            # Load all prediction files for the specified model
            prediction_files = list(self.predictions_dir.glob("prediction_sequences_*.json"))
            if prediction_files:
                print(f"\nAnalyzing prediction files for model: {self.model_path or 'all'}")
                loaded_files = 0
                for file in prediction_files:
                    with open(file) as f:
                        data = json.load(f)
                        stored_model = data["metadata"].get("model_path", "")
                        if self.model_path is None or stored_model == self.model_path:
                            print(f"Loading predictions from: {file.name}")
                            self.prediction_sequences.append(data)
                            loaded_files += 1
                        else:
                            print(f"Skipping {file.name} - generated by different model: {stored_model}")
                if loaded_files == 0:
                    print(f"\nWarning: No prediction files found for model: {self.model_path}")
                    print("Available model paths in predictions:")
                    for file in prediction_files:
                        with open(file) as f:
                            data = json.load(f)
                            print(f"  - {data['metadata'].get('model_path', 'unknown')}")
    
    def analyze_fall_patterns(self) -> pd.DataFrame:
        """Analyze patterns in fall sequences vs non-fall sequences."""
        patterns = []
        
        for data in self.recorded_sequences:
            for sequence in data["sequences"]:
                frames = np.array([frame["frame"] for frame in sequence["frames"]])
                
                # Calculate key metrics for fall detection
                max_activation = np.max(frames)
                mean_activation = np.mean(frames)
                std_activation = np.std(frames)
                
                # Calculate motion metrics
                frame_diffs = np.diff(frames, axis=0)
                motion_magnitude = np.mean(np.abs(frame_diffs))
                peak_motion = np.max(np.abs(frame_diffs))
                
                # Calculate spatial distribution
                center_of_mass_y = []
                center_of_mass_x = []
                for frame in frames:
                    if np.sum(frame) > 0:
                        y_coords, x_coords = np.where(frame > 0)
                        center_of_mass_y.append(np.mean(y_coords))
                        center_of_mass_x.append(np.mean(x_coords))
                
                vertical_displacement = np.std(center_of_mass_y) if center_of_mass_y else 0
                horizontal_displacement = np.std(center_of_mass_x) if center_of_mass_x else 0
                
                patterns.append({
                    'label': sequence.get('label', 'unknown'),
                    'max_activation': max_activation,
                    'mean_activation': mean_activation,
                    'std_activation': std_activation,
                    'motion_magnitude': motion_magnitude,
                    'peak_motion': peak_motion,
                    'vertical_displacement': vertical_displacement,
                    'horizontal_displacement': horizontal_displacement,
                    'sequence_length': len(frames),
                    'timestamp': sequence.get('timestamp', '')
                })
        
        return pd.DataFrame(patterns)
    
    def analyze_model_performance(self) -> Tuple[pd.DataFrame, Dict[str, float]]:
        """Analyze model prediction accuracy and patterns."""
        predictions = []
        confusion_matrix = {
            'true_positive_count': 0,
            'false_positive_count': 0,
            'missed_fall_count': 0,
            'correct_normal_count': 0
        }
        
        for data in self.prediction_sequences:
            for sequence in data["sequences"]:
                label = sequence.get('label', 'unknown')
                if label in ['true_positive', 'false_positive', 'missed_fall', 'correct_normal']:
                    confusion_matrix[f'{label}_count'] += 1
                
                # Analyze prediction confidence
                if 'predictions' in sequence:
                    probs = [p['probability'] for p in sequence['predictions']]
                    predictions.append({
                        'label': label,
                        'mean_confidence': np.mean(probs),
                        'max_confidence': np.max(probs),
                        'min_confidence': np.min(probs),
                        'std_confidence': np.std(probs),
                        'sequence_length': len(sequence['frames']),
                        'timestamp': sequence.get('timestamp', '')
                    })
        
        # Calculate performance metrics
        total = sum(confusion_matrix.values())
        if total > 0:
            metrics = {
                'accuracy': (confusion_matrix['true_positive_count'] + confusion_matrix['correct_normal_count']) / total,
                'fall_detection_rate': confusion_matrix['true_positive_count'] / 
                    (confusion_matrix['true_positive_count'] + confusion_matrix['missed_fall_count']) 
                    if (confusion_matrix['true_positive_count'] + confusion_matrix['missed_fall_count']) > 0 else 0,
                'false_alarm_rate': confusion_matrix['false_positive_count'] / 
                    (confusion_matrix['false_positive_count'] + confusion_matrix['correct_normal_count'])
                    if (confusion_matrix['false_positive_count'] + confusion_matrix['correct_normal_count']) > 0 else 0
            }
        else:
            metrics = {'accuracy': 0, 'fall_detection_rate': 0, 'false_alarm_rate': 0}
            
        return pd.DataFrame(predictions), {**metrics, **confusion_matrix}
    
    def analyze_tagged_sequences(self) -> pd.DataFrame:
        """Analyze and compare tagged sequences with their predictions."""
        analysis = []
        
        # Create a mapping of timestamps to predictions for quick lookup
        prediction_map = {}
        for pred_data in self.prediction_sequences:
            for sequence in pred_data["sequences"]:
                if "timestamp" in sequence:
                    prediction_map[sequence["timestamp"]] = sequence

        # Process training data
        for rec_data in self.recorded_sequences:
            for sequence in rec_data["sequences"]:
                timestamp = sequence.get("timestamp")
                if not timestamp:
                    continue
                    
                # Extract sequence characteristics
                frames = np.array([frame["frame"] for frame in sequence["frames"]])
                
                # Calculate motion metrics
                frame_diffs = np.diff(frames, axis=0)
                motion_magnitude = np.mean(np.abs(frame_diffs))
                
                # Calculate vertical movement (important for fall detection)
                center_of_mass_y = []
                for frame in frames:
                    if np.sum(frame) > 0:
                        y_coords, _ = np.where(frame > 0)
                        center_of_mass_y.append(np.mean(y_coords))
                vertical_movement = np.std(center_of_mass_y) if center_of_mass_y else 0
                
                # Basic sequence metrics
                sequence_data = {
                    'timestamp': timestamp,
                    'training_label': sequence.get('label', 'unknown'),
                    'sequence_length': len(frames),
                    'motion_magnitude': motion_magnitude,
                    'vertical_movement': vertical_movement,
                    'max_activation': np.max(frames),
                    'mean_activation': np.mean(frames),
                    'notes': sequence.get('notes', '')
                }
                
                # Add prediction data if available
                pred_sequence = prediction_map.get(timestamp)
                if pred_sequence:
                    probs = [p['probability'] for p in pred_sequence.get('predictions', [])]
                    sequence_data.update({
                        'prediction_tag': pred_sequence.get('label', 'unknown'),
                        'mean_confidence': np.mean(probs) if probs else 0,
                        'max_confidence': np.max(probs) if probs else 0
                    })
                    
                    # Determine if prediction was correct based on tags
                    pred_tag = pred_sequence.get('label', '')
                    if pred_tag == 'correct_normal' and sequence_data['training_label'] in ['walk', 'normal', 'object']:
                        sequence_data['prediction_correct'] = True
                    elif pred_tag == 'true_positive' and sequence_data['training_label'] == 'fall':
                        sequence_data['prediction_correct'] = True
                    elif pred_tag == 'false_positive' and sequence_data['training_label'] != 'fall':
                        sequence_data['prediction_correct'] = False
                    elif pred_tag == 'missed_fall' and sequence_data['training_label'] == 'fall':
                        sequence_data['prediction_correct'] = False
                    else:
                        sequence_data['prediction_correct'] = False
                else:
                    sequence_data.update({
                        'prediction_tag': 'no_prediction',
                        'mean_confidence': 0,
                        'max_confidence': 0,
                        'prediction_correct': False
                    })
                
                analysis.append(sequence_data)
        
        return pd.DataFrame(analysis)

    def plot_sequence_analysis(self, data: pd.DataFrame):
        """Create comprehensive visualization of sequence analysis."""
        plt.style.use('default')
        fig = plt.figure(figsize=(20, 15))
        gs = fig.add_gridspec(3, 2)
        
        # 1. Training Labels vs Prediction Tags Matrix
        ax1 = fig.add_subplot(gs[0, 0])
        confusion_data = pd.crosstab(data['training_label'], data['prediction_tag'])
        sns.heatmap(confusion_data, annot=True, fmt='d', ax=ax1)
        ax1.set_title('Training Labels vs Prediction Tags')
        
        # 2. Motion Characteristics by Training Label
        ax2 = fig.add_subplot(gs[0, 1])
        sns.scatterplot(data=data, x='motion_magnitude', y='vertical_movement',
                       hue='training_label', style='prediction_correct', ax=ax2)
        ax2.set_title('Motion Characteristics by Training Label')
        
        # 3. Confidence Distribution by Training Label
        ax3 = fig.add_subplot(gs[1, 0])
        sns.boxplot(data=data, x='training_label', y='mean_confidence', ax=ax3)
        ax3.set_title('Model Confidence by Training Label')
        ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45)
        
        # 4. Confidence Distribution by Prediction Tag
        ax4 = fig.add_subplot(gs[1, 1])
        sns.boxplot(data=data, x='prediction_tag', y='mean_confidence', ax=ax4)
        ax4.set_title('Model Confidence by Prediction Tag')
        ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45)
        
        # 5. Time Series of Predictions
        ax5 = fig.add_subplot(gs[2, :])
        data['datetime'] = pd.to_datetime(data['timestamp'])
        data = data.sort_values('datetime')
        
        # Plot confidence over time with color-coded correctness
        scatter = ax5.scatter(data['datetime'], data['mean_confidence'],
                            c=data['prediction_correct'].map({True: 'green', False: 'red'}),
                            alpha=0.6)
        ax5.set_title('Prediction Confidence Over Time')
        ax5.set_xlabel('Time')
        ax5.set_ylabel('Confidence')
        
        # Add legend
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                    markerfacecolor=c, label=l, markersize=10)
                         for c, l in zip(['green', 'red'], ['Correct', 'Incorrect'])]
        ax5.legend(handles=legend_elements)
        
        plt.tight_layout()
        plt.show()

    def generate_analysis_report(self, data: pd.DataFrame) -> str:
        """Generate a detailed analysis report with narrative summary."""
        total_sequences = len(data)
        correct_predictions = data['prediction_correct'].sum()
        
        # Calculate key metrics for summary
        accuracy = (correct_predictions/total_sequences)*100 if total_sequences > 0 else 0
        fall_sequences = data[data['training_label'] == 'fall']
        non_fall_sequences = data[data['training_label'] != 'fall']
        
        detected_falls = fall_sequences[fall_sequences['prediction_tag'] == 'true_positive'].shape[0]
        missed_falls = fall_sequences[fall_sequences['prediction_tag'] == 'missed_fall'].shape[0]
        false_positives = data[data['prediction_tag'] == 'false_positive'].shape[0]
        
        fall_detection_rate = (detected_falls / len(fall_sequences))*100 if len(fall_sequences) > 0 else 0
        false_positive_rate = (false_positives / len(non_fall_sequences))*100 if len(non_fall_sequences) > 0 else 0
        
        # Calculate confidence patterns
        high_confidence_correct = data[(data['prediction_correct']) & (data['mean_confidence'] > 0.8)].shape[0]
        high_confidence_incorrect = data[(~data['prediction_correct']) & (data['mean_confidence'] > 0.8)].shape[0]
        
        report = [
            "Fall Detection Analysis Report",
            "=" * 30,
            "\nExecutive Summary:",
            "-" * 17,
        ]
        
        # Generate narrative summary
        summary = [
            f"Analysis of {total_sequences} sequences shows an overall accuracy of {accuracy:.1f}%.",
        ]
        
        # Fall detection performance
        if len(fall_sequences) > 0:
            summary.append(
                f"The system successfully detected {detected_falls} out of {len(fall_sequences)} falls "
                f"({fall_detection_rate:.1f}% detection rate)."
            )
            if missed_falls > 0:
                summary.append(f"However, it missed {missed_falls} falls, which requires attention.")
        else:
            summary.append("No fall sequences were present in the analyzed data.")
            
        # False positive analysis
        if false_positives > 0:
            summary.append(
                f"The system generated {false_positives} false positives "
                f"({false_positive_rate:.1f}% of non-fall activities)."
            )
        else:
            summary.append("The system did not generate any false positives.")
            
        # Confidence analysis
        if high_confidence_correct > 0 or high_confidence_incorrect > 0:
            summary.append(
                f"\nAnalyzing prediction confidence: {high_confidence_correct} correct predictions and "
                f"{high_confidence_incorrect} incorrect predictions were made with high confidence (>80%)."
            )
            
        # Motion pattern analysis
        fall_motion = data[data['training_label'] == 'fall']['motion_magnitude'].mean()
        normal_motion = data[data['training_label'] != 'fall']['motion_magnitude'].mean()
        if not pd.isna(fall_motion) and not pd.isna(normal_motion):
            motion_diff = (fall_motion - normal_motion) / normal_motion * 100
            summary.append(
                f"\nMotion analysis shows fall events have {abs(motion_diff):.1f}% "
                f"{'higher' if motion_diff > 0 else 'lower'} motion magnitude compared to normal activities."
            )
            
        # Add recommendations
        summary.append("\nKey Recommendations:")
        if missed_falls > 0:
            summary.append("- Review missed falls to identify common patterns that the system fails to detect.")
        if false_positives > 0:
            summary.append("- Analyze false positives to understand what movements are being misclassified as falls.")
        if high_confidence_incorrect > 0:
            summary.append("- Investigate high-confidence incorrect predictions to improve the model's certainty threshold.")
        
        # Add summary to report
        report.extend(summary)
        
        # Add detailed statistics
        report.extend([
            "\nDetailed Statistics:",
            "-" * 19,
            f"\nTotal Sequences Analyzed: {total_sequences}",
            f"Correct Predictions: {correct_predictions} ({accuracy:.1f}%)",
            "\nBreakdown by Training Label:",
        ])
        
        # Training label distribution
        label_dist = data['training_label'].value_counts()
        for label, count in label_dist.items():
            correct = data[(data['training_label'] == label) & (data['prediction_correct'])].shape[0]
            report.append(f"\n{label}:")
            report.append(f"  Total: {count}")
            report.append(f"  Correctly Predicted: {correct} ({(correct/count)*100:.1f}%)")
            
        report.append("\nBreakdown by Prediction Tag:")
        tag_dist = data['prediction_tag'].value_counts()
        for tag, count in tag_dist.items():
            correct = data[(data['prediction_tag'] == tag) & (data['prediction_correct'])].shape[0]
            report.append(f"\n{tag}:")
            report.append(f"  Total: {count}")
            report.append(f"  Correct Classifications: {correct} ({(correct/count)*100:.1f}%)")
            
        # Confidence analysis
        report.extend([
            "\nConfidence Analysis:",
            "-" * 19,
            f"Mean Confidence: {data['mean_confidence'].mean():.2f}",
            f"Confidence in Correct Predictions: {data[data['prediction_correct']]['mean_confidence'].mean():.2f}",
            f"Confidence in Incorrect Predictions: {data[~data['prediction_correct']]['mean_confidence'].mean():.2f}"
        ])
        
        # Motion characteristics by label
        report.append("\nMotion Characteristics by Label:")
        report.append("-" * 28)
        for label in data['training_label'].unique():
            label_data = data[data['training_label'] == label]
            report.extend([
                f"\n{label}:",
                f"  Average Motion Magnitude: {label_data['motion_magnitude'].mean():.2f}",
                f"  Average Vertical Movement: {label_data['vertical_movement'].mean():.2f}"
            ])
        
        return "\n".join(report)

    def plot_fall_characteristics(self, data: pd.DataFrame):
        """Plot key characteristics of fall vs non-fall sequences."""
        fig, axes = plt.subplots(2, 2, figsize=(15, 15))
        fig.suptitle('Fall vs Non-Fall Characteristics', fontsize=16)
        
        # Motion magnitude vs vertical displacement
        sns.scatterplot(data=data, x='motion_magnitude', y='vertical_displacement',
                       hue='label', ax=axes[0,0])
        axes[0,0].set_title('Motion Pattern Analysis')
        
        # Activation patterns
        sns.boxplot(data=data, x='label', y='max_activation', ax=axes[0,1])
        axes[0,1].set_title('Peak Activation by Type')
        
        # Motion over time
        sns.boxplot(data=data, x='label', y='motion_magnitude', ax=axes[1,0])
        axes[1,0].set_title('Motion Magnitude by Type')
        
        # Displacement patterns
        sns.boxplot(data=data, x='label', y='vertical_displacement', ax=axes[1,1])
        axes[1,1].set_title('Vertical Displacement by Type')
        
        plt.tight_layout()
        plt.show()

    def visualize_sequence_comparison(self, seq_idx: int = 0, file_idx: int = 0):
        """Create side-by-side visualization of recorded vs predicted sequence."""
        if not (self.recorded_sequences and self.prediction_sequences):
            print("No sequences loaded for comparison")
            return
            
        recorded_seq = self.recorded_sequences[file_idx]["sequences"][seq_idx]
        pred_seq = self.prediction_sequences[file_idx]["sequences"][seq_idx]
        
        # Ensure both sequences have the same number of frames
        n_frames = min(len(recorded_seq["frames"]), len(pred_seq["frames"]))
        if n_frames == 0:
            print("Error: No frames available for visualization")
            return None
            
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))
        
        def update(frame_idx):
            ax1.clear()
            ax2.clear()
            
            # Plot recorded frame
            recorded_frame = np.array(recorded_seq["frames"][frame_idx]["frame"])
            sns.heatmap(recorded_frame, ax=ax1, cmap='YlOrRd', vmin=0, vmax=1)
            ax1.set_title(f'Recorded\nLabel: {recorded_seq["label"]}')
            
            # Plot predicted frame
            predicted_frame = np.array(pred_seq["frames"][frame_idx]["frame"])
            sns.heatmap(predicted_frame, ax=ax2, cmap='YlOrRd', vmin=0, vmax=1)
            prob = pred_seq["predictions"][frame_idx]["probability"] if "predictions" in pred_seq else "N/A"
            ax2.set_title(f'Predicted\nConfidence: {prob:.2f}')
            
            plt.tight_layout()
            return [ax1, ax2]
        
        ani = animation.FuncAnimation(
            fig, update, frames=n_frames,
            interval=100, repeat=True
        )
        plt.show()
        return ani

    def analyze_activity_characteristics(self) -> pd.DataFrame:
        """Perform detailed analysis of characteristics that distinguish falls from walks and objects.
        
        Analyzes recorded sequences to extract key differentiating features between
        falls, walks, and placed objects. This helps identify the most reliable
        indicators for fall detection and object discrimination.
        
        Returns:
            DataFrame containing detailed metrics for each sequence including:
            - Basic metrics (motion magnitude, vertical movement)
            - Temporal characteristics (duration, peak timing)
            - Spatial characteristics (coverage area, distribution)
            - Movement patterns (direction changes, acceleration)
            - Progression metrics (pattern evolution, coherence)
        """
        characteristics = []
        window_size = 5  # Half second at 10Hz sampling rate
        
        for data in self.recorded_sequences:
            for sequence in data["sequences"]:
                frames = np.array([frame["frame"] for frame in sequence["frames"]])
                frame_count = len(frames)
                
                if sequence.get('label') not in ['fall', 'walk', 'object']:
                    continue
                
                if frame_count < window_size:
                    continue
                
                # Calculate basic motion metrics
                frame_diffs = np.diff(frames, axis=0)
                motion_magnitude = np.mean(np.abs(frame_diffs))
                
                # Temporal characteristics
                peak_frame = np.argmax([np.sum(frame) for frame in frames])
                time_to_peak = peak_frame / frame_count
                
                # Spatial characteristics
                coverage_areas = []
                center_of_mass_positions = []
                
                for frame in frames:
                    if np.sum(frame) > 0:
                        y_coords, x_coords = np.where(frame > 0)
                        coverage_areas.append(len(x_coords))
                        center_of_mass_positions.append([np.mean(y_coords), np.mean(x_coords)])
                
                com_positions = np.array(center_of_mass_positions) if center_of_mass_positions else np.array([[0, 0]])
                
                # Movement patterns
                vertical_movement = np.std(com_positions[:, 0]) if len(com_positions) > 1 else 0
                horizontal_movement = np.std(com_positions[:, 1]) if len(com_positions) > 1 else 0
                
                # Calculate velocity and acceleration
                velocities = []
                if len(com_positions) > 1:
                    velocities = np.sqrt(np.sum(np.diff(com_positions, axis=0)**2, axis=1))
                
                max_velocity = float(np.max(velocities)) if len(velocities) > 0 else 0
                mean_velocity = float(np.mean(velocities)) if len(velocities) > 0 else 0
                
                # Coverage characteristics
                max_coverage = np.max(coverage_areas) if coverage_areas else 0
                mean_coverage = np.mean(coverage_areas) if coverage_areas else 0
                coverage_variation = np.std(coverage_areas) if coverage_areas else 0
                
                # Calculate new progression metrics across windows
                progression_metrics = []
                for i in range(frame_count - window_size + 1):
                    window_metrics = self._calculate_window_metrics(frames[i:i+window_size])
                    if window_metrics:
                        progression_metrics.append(window_metrics)
                
                if not progression_metrics:
                    continue
                
                # Calculate summary statistics for progression metrics
                summary_metrics = {}
                for key in progression_metrics[0].keys():
                    values = [m[key] for m in progression_metrics]
                    summary_metrics.update({
                        f'{key}_mean': np.mean(values),
                        f'{key}_max': np.max(values),
                        f'{key}_min': np.min(values),
                        f'{key}_std': np.std(values)
                    })
                
                # Calculate fall vs object indicators
                fall_indicator = (
                    summary_metrics['vertical_speed_mean'] * 
                    summary_metrics['expansion_rate_max'] * 
                    (1 - abs(summary_metrics['pattern_coherence_mean']))
                )
                
                object_indicator = (
                    summary_metrics['height_stability_mean'] * 
                    summary_metrics['width_stability_mean'] * 
                    abs(summary_metrics['pattern_coherence_mean'])
                )
                
                # Combine all metrics
                characteristics.append({
                    'label': sequence.get('label'),
                    'timestamp': sequence.get('timestamp'),
                    'sequence_length': frame_count,
                    'motion_magnitude': motion_magnitude,
                    'vertical_movement': vertical_movement,
                    'horizontal_movement': horizontal_movement,
                    'max_velocity': max_velocity,
                    'mean_velocity': mean_velocity,
                    'max_coverage': max_coverage,
                    'mean_coverage': mean_coverage,
                    'coverage_variation': coverage_variation,
                    'time_to_peak': time_to_peak,
                    **summary_metrics,
                    'fall_indicator': fall_indicator,
                    'object_indicator': object_indicator,
                    'notes': sequence.get('notes', '')
                })
        
        return pd.DataFrame(characteristics)

    def generate_characteristic_report(self, data: pd.DataFrame) -> str:
        """Generate a detailed report comparing fall vs walk characteristics."""
        fall_data = data[data['label'] == 'fall']
        walk_data = data[data['label'] == 'walk']
        
        if len(fall_data) == 0 or len(walk_data) == 0:
            return "Insufficient data: Need both fall and walk sequences for comparison"
        
        report = [
            "Fall vs Walk Characteristic Analysis",
            "=" * 35,
            f"\nAnalysis based on {len(fall_data)} fall sequences and {len(walk_data)} walk sequences\n"
        ]
        
        # Calculate and report differences for each metric
        metrics = [
            ('motion_magnitude', 'Motion Magnitude'),
            ('vertical_movement', 'Vertical Movement'),
            ('horizontal_movement', 'Horizontal Movement'),
            ('max_velocity', 'Maximum Velocity'),
            ('mean_velocity', 'Average Velocity'),
            ('max_coverage', 'Maximum Sensor Coverage'),
            ('mean_coverage', 'Average Sensor Coverage'),
            ('coverage_variation', 'Coverage Variation'),
            ('time_to_peak', 'Time to Peak Activity')
        ]
        
        report.append("\nKey Differentiating Characteristics:")
        report.append("-" * 31)
        
        for metric, label in metrics:
            fall_mean = fall_data[metric].mean()
            walk_mean = walk_data[metric].mean()
            
            if fall_mean == 0 and walk_mean == 0:
                continue
                
            difference = ((fall_mean - walk_mean) / walk_mean * 100) if walk_mean != 0 else float('inf')
            
            report.append(f"\n{label}:")
            report.append(f"  Fall sequences: {fall_mean:.2f}")
            report.append(f"  Walk sequences: {walk_mean:.2f}")
            report.append(f"  Difference: {abs(difference):.1f}% {'higher' if difference > 0 else 'lower'} in falls")
            
            # Add statistical significance if we have enough samples
            if len(fall_data) >= 5 and len(walk_data) >= 5:
                from scipy import stats
                t_stat, p_value = stats.ttest_ind(fall_data[metric], walk_data[metric])
                if p_value < 0.05:
                    report.append(f"  Statistical Significance: Significant (p={p_value:.3f})")
        
        # Add sequence duration analysis
        report.append("\nSequence Duration Analysis:")
        report.append("-" * 24)
        fall_duration = fall_data['sequence_length'].mean()
        walk_duration = walk_data['sequence_length'].mean()
        duration_diff = ((fall_duration - walk_duration) / walk_duration * 100)
        report.append(f"Fall sequences are {abs(duration_diff):.1f}% {'longer' if duration_diff > 0 else 'shorter'} than walks")
        
        return "\n".join(report)

    def analyze_post_fall_patterns(self) -> pd.DataFrame:
        """Analyze activity patterns in the seconds following a detected fall.
        
        This analysis helps distinguish true falls from false positives by examining
        the characteristic lack of normal walking patterns after a fall event.
        """
        post_fall_patterns = []
        
        for data in self.recorded_sequences:
            for sequence in data["sequences"]:
                if sequence.get('label') not in ['fall', 'walk']:
                    continue
                    
                frames = np.array([frame["frame"] for frame in sequence["frames"]])
                frame_count = len(frames)
                
                # Find frame with peak activity (potential fall moment)
                peak_frame = np.argmax([np.sum(frame) for frame in frames])
                
                # Analyze post-peak frames (if we have at least 5 seconds of data after peak)
                if frame_count - peak_frame >= 10:  # Assuming 2Hz sampling rate
                    post_peak_frames = frames[peak_frame:peak_frame+10]
                    
                    # Calculate activity metrics for post-peak period
                    post_peak_motion = []
                    post_peak_coverage = []
                    post_peak_positions = []
                    
                    for frame in post_peak_frames:
                        if np.sum(frame) > 0:
                            y_coords, x_coords = np.where(frame > 0)
                            post_peak_coverage.append(len(x_coords))
                            post_peak_positions.append([np.mean(y_coords), np.mean(x_coords)])
                    
                    # Calculate post-fall characteristics
                    post_fall_patterns.append({
                        'label': sequence.get('label'),
                        'timestamp': sequence.get('timestamp'),
                        'frames_analyzed': len(post_peak_frames),
                        'avg_post_coverage': np.mean(post_peak_coverage) if post_peak_coverage else 0,
                        'coverage_stability': np.std(post_peak_coverage) if post_peak_coverage else 0,
                        'position_stability': np.std([p[0] for p in post_peak_positions]) if post_peak_positions else 0,
                        'has_walking_pattern': self._detect_walking_pattern(post_peak_frames),
                        'recovery_time': self._estimate_recovery_time(post_peak_frames),
                        'notes': sequence.get('notes', '')
                    })
        
        return pd.DataFrame(post_fall_patterns)
    
    def _detect_walking_pattern(self, frames: np.ndarray) -> bool:
        """Detect if frames contain regular walking patterns."""
        # Calculate frame-to-frame changes
        frame_diffs = np.diff(frames, axis=0)
        
        if len(frame_diffs) < 4:  # Need at least 4 frames to detect pattern
            return False
            
        # Look for alternating patterns characteristic of walking
        activations = [np.sum(np.abs(diff)) for diff in frame_diffs]
        
        # Check for regular periodicity in activations
        if len(activations) >= 4:
            # Walking typically shows regular peaks every 2-3 frames
            peaks = [i for i in range(1, len(activations)-1) if 
                    activations[i] > activations[i-1] and 
                    activations[i] > activations[i+1]]
            
            if peaks:
                peak_distances = np.diff(peaks)
                # Check if peaks occur at regular intervals (characteristic of walking)
                return np.std(peak_distances) < 1.0
        
        return False
    
    def _estimate_recovery_time(self, frames: np.ndarray) -> float:
        """Estimate time until significant movement resumes after peak activity."""
        frame_activities = [np.sum(frame) for frame in frames]
        
        # Look for sustained period of low activity followed by increased movement
        low_activity_threshold = 0.3 * max(frame_activities)
        
        recovery_frame = len(frames)  # Default to maximum if no recovery detected
        for i in range(1, len(frame_activities)):
            if frame_activities[i] > low_activity_threshold:
                # Check if this is sustained movement
                if i + 2 < len(frame_activities) and \
                   frame_activities[i+1] > low_activity_threshold and \
                   frame_activities[i+2] > low_activity_threshold:
                    recovery_frame = i
                    break
        
        return recovery_frame / 2.0  # Convert frames to seconds (assuming 2Hz)

    def generate_post_fall_report(self, data: pd.DataFrame) -> str:
        """Generate a report analyzing post-fall activity patterns."""
        fall_data = data[data['label'] == 'fall']
        walk_data = data[data['label'] == 'walk']
        
        if len(fall_data) == 0 or len(walk_data) == 0:
            return "Insufficient data: Need both fall and walk sequences for comparison"
        
        report = [
            "Post-Fall Activity Analysis",
            "=" * 24,
            f"\nAnalysis based on {len(fall_data)} fall sequences and {len(walk_data)} walk sequences\n"
        ]
        
        # Analyze post-peak characteristics
        report.append("\nPost-Peak Activity Patterns:")
        report.append("-" * 25)
        
        # Walking pattern detection
        falls_with_walking = fall_data['has_walking_pattern'].sum()
        walks_with_walking = walk_data['has_walking_pattern'].sum()
        
        report.append(f"\nWalking Pattern Detection:")
        report.append(f"  Falls showing walking patterns: {falls_with_walking}/{len(fall_data)} ({falls_with_walking/len(fall_data)*100:.1f}%)")
        report.append(f"  Walks showing walking patterns: {walks_with_walking}/{len(walk_data)} ({walks_with_walking/len(walk_data)*100:.1f}%)")
        
        # Recovery time analysis
        fall_recovery = fall_data['recovery_time'].mean()
        walk_recovery = walk_data['recovery_time'].mean()
        
        report.append(f"\nRecovery Time Analysis:")
        report.append(f"  Average recovery time after falls: {fall_recovery:.1f} seconds")
        report.append(f"  Average recovery time after walks: {walk_recovery:.1f} seconds")
        
        # Position stability
        fall_stability = fall_data['position_stability'].mean()
        walk_stability = walk_data['position_stability'].mean()
        
        report.append(f"\nPosition Stability:")
        report.append(f"  Fall sequences: {fall_stability:.2f}")
        report.append(f"  Walk sequences: {walk_stability:.2f}")
        
        # Add recommendations
        report.append("\nRecommendations for Fall Detection:")
        report.append("-" * 31)
        report.append("1. Look for absence of walking patterns after peak activity")
        report.append(f"2. Expect limited movement for at least {fall_recovery:.1f} seconds after a fall")
        report.append("3. Monitor position stability for sudden changes")
        
        return "\n".join(report)

    def _get_pattern_characteristics(self, frame):
        """Extract spatial characteristics from a single frame.
        
        Parameters:
        -----------
        frame : np.array
            Binary grid state (12x15)
            
        Returns:
        --------
        dict or None
            Spatial characteristics including height, width, center of mass,
            and number of activated sensors. Returns None if no sensors are activated.
        """
        activated = np.argwhere(frame)
        if len(activated) == 0:
            return None
            
        min_row, min_col = activated.min(axis=0)
        max_row, max_col = activated.max(axis=0)
        com = activated.mean(axis=0)
        
        return {
            'height': max_row - min_row + 1,
            'width': max_col - min_col + 1,
            'com_y': com[0],
            'com_x': com[1],
            'num_activated': len(activated)
        }
        
    def _calculate_window_metrics(self, window_frames):
        """Calculate progression metrics for a window of frames.
        
        Parameters:
        -----------
        window_frames : list of np.array
            Sequence of binary grid states to analyze
            
        Returns:
        --------
        dict or None
            Progression metrics including speeds, changes, and stability measures.
            Returns None if insufficient valid frames in window.
        """
        window_chars = []
        for frame in window_frames:
            chars = self._get_pattern_characteristics(frame)
            if chars:
                window_chars.append(chars)
                
        if len(window_chars) < 2:
            return None
            
        # Extract sequences of characteristics
        heights = np.array([c['height'] for c in window_chars])
        widths = np.array([c['width'] for c in window_chars])
        coms_y = np.array([c['com_y'] for c in window_chars])
        activations = np.array([c['num_activated'] for c in window_chars])
        
        # Calculate metrics
        metrics = {
            # Vertical movement characteristics
            'vertical_speed': np.mean(np.diff(coms_y)),
            'vertical_acceleration': np.mean(np.diff(np.diff(coms_y))) if len(coms_y) > 2 else 0,
            
            # Pattern size evolution
            'height_change_rate': np.mean(np.diff(heights)),
            'width_change_rate': np.mean(np.diff(widths)),
            'size_ratio_change': np.mean(np.diff(heights/widths)),
            
            # Activation pattern changes
            'activation_change_rate': np.mean(np.diff(activations)),
            'activation_stability': np.std(activations),
            
            # Pattern consistency
            'height_stability': np.std(heights),
            'width_stability': np.std(widths),
            
            # Composite metrics
            'expansion_rate': np.mean(np.diff(heights * widths)),
            'pattern_coherence': np.corrcoef(heights, widths)[0,1]
        }
        
        return metrics

def main():
    parser = argparse.ArgumentParser(description='Analyze fall detection data')
    parser.add_argument('--data-dir', type=str, default='data',
                      help='Directory containing recorded sequences')
    parser.add_argument('--predictions-dir', type=str, default='predictions',
                      help='Directory containing prediction sequences')
    parser.add_argument('--recorded-file', type=str, default=None,
                      help='Specific recorded sequence file to analyze')
    parser.add_argument('--prediction-file', type=str, default=None,
                      help='Specific prediction sequence file to analyze')
    parser.add_argument('--model-path', type=str, default='models/fall_detector.pkl',
                      help='Path to the model used for predictions')
    parser.add_argument('--analyze-characteristics', action='store_true',
                      help='Perform detailed analysis of sequence characteristics')
    parser.add_argument('--compare', nargs='+', choices=['fall', 'walk', 'object', 'all'],
                      default=['all'],
                      help='Specify which sequence types to compare (e.g., --compare fall object)')
    parser.add_argument('--visualize', action='store_true',
                      help='Show sequence visualizations')
    parser.add_argument('--report', action='store_true',
                      help='Generate detailed analysis report')
    
    args = parser.parse_args()
    
    print("\nFall Detection Analysis Workflow:")
    print("1. Collect Training Data (live_monitor.py):")
    print("   - Record sequences with labels (walk, fall, object, etc.)")
    print("   - Data saved to: /data/recorded_sequences_[timestamp].json")
    print("\n2. Test Model (live_predictor.py):")
    print("   - Make predictions on new sequences")
    print("   - Tag predictions as:")
    print("     T: true_positive  (correctly detected fall)")
    print("     F: false_positive (incorrectly detected fall)")
    print("     M: missed_fall    (missed actual fall)")
    print("     C: correct_normal (correctly identified normal activity)")
    print("   - Predictions saved to: /predictions/prediction_sequences_[timestamp].json")
    print("\n3. Analyze Results (fall_analysis.py):")
    print("   - Compare training data with prediction tags")
    print("   - Generate visualizations and statistics")
    print("   - Analyze sequence characteristics")
    print("   - Provide recommendations for improvement")
    
    analyzer = FallAnalyzer(args.data_dir, args.predictions_dir, args.model_path)
    analyzer.load_data(args.recorded_file, args.prediction_file)
    
    # Analyze fall patterns
    print("\nFall Pattern Analysis:")
    fall_patterns = analyzer.analyze_fall_patterns()
    print(fall_patterns.describe())
    
    # Analyze model performance if prediction data is available
    if args.prediction_file:
        print("\nModel Performance Analysis:")
        print("=" * 80)
        predictions, metrics = analyzer.analyze_model_performance()
        
        print("\nDetailed Counts:")
        print(f"True Positives (correctly detected falls): {metrics.get('true_positive_count', 0)}")
        print(f"False Positives (incorrectly detected falls): {metrics.get('false_positive_count', 0)}")
        print(f"Missed Falls (undetected falls): {metrics.get('missed_fall_count', 0)}")
        print(f"Correct Normal (correctly identified non-falls): {metrics.get('correct_normal_count', 0)}")
        
        print("\nPerformance Metrics:")
        total_actual_falls = metrics.get('true_positive_count', 0) + metrics.get('missed_fall_count', 0)
        total_predicted_falls = metrics.get('true_positive_count', 0) + metrics.get('false_positive_count', 0)
        
        print(f"Total Actual Falls: {total_actual_falls}")
        print(f"Total Predicted Falls: {total_predicted_falls}")
        print(f"Fall Detection Rate: {metrics.get('fall_detection_rate', 0):.2%}")
        print(f"False Alarm Rate: {metrics.get('false_alarm_rate', 0):.2%}")
        print(f"Overall Accuracy: {metrics.get('accuracy', 0):.2%}")
        
        # Analyze tagged sequences
        analysis_data = analyzer.analyze_tagged_sequences()
        if not analysis_data.empty:
            analyzer.plot_sequence_analysis(analysis_data)
        
        if args.report:
            report = analyzer.generate_analysis_report(analysis_data)
            print("\n" + report)
        else:
            print("\nNo matching sequences found between training data and predictions.")
            print("To analyze model performance:")
            print("1. Use live_monitor.py to record training sequences")
            print("2. Use live_predictor.py to make and tag predictions")
            print("3. Run this analysis tool again with matching sequence files")
            print("\nExample:")
            print("python scripts/fall_analysis.py --recorded-file data/recorded_sequences_20241125_163415.json --prediction-file predictions/prediction_sequences_20241125_163905.json")
    
    if args.visualize:
        print("\nPress Ctrl+C to stop the animation and proceed")
        try:
            analyzer.visualize_sequence_comparison()
        except KeyboardInterrupt:
            plt.close()
    
    # Plot fall characteristics
    analyzer.plot_fall_characteristics(fall_patterns)
    
    # Analyze activity characteristics
    activity_characteristics = analyzer.analyze_activity_characteristics()
    print("\nActivity Characteristics:")
    print(activity_characteristics.describe())
    
    # Generate characteristic report
    characteristic_report = analyzer.generate_characteristic_report(activity_characteristics)
    print("\n" + characteristic_report)
    
    # Analyze characteristics if requested
    if args.analyze_characteristics:
        sequence_types = args.compare
        if 'all' in sequence_types:
            sequence_types = ['fall', 'walk', 'object']
        
        filtered_df = activity_characteristics[activity_characteristics['label'].isin(sequence_types)]
        
        # Add sequence count reporting
        for seq_type in sequence_types:
            count = len(filtered_df[filtered_df['label'] == seq_type])
            print(f"\nFound {count} {seq_type} sequences")
        
        if filtered_df.empty:
            print(f"\nNo sequences found for types: {sequence_types}")
            return
        
        # Require at least 2 sequences of each type for meaningful comparison
        insufficient_data = False
        for seq_type in sequence_types:
            count = len(filtered_df[filtered_df['label'] == seq_type])
            if count < 2:
                print(f"\nWarning: Need at least 2 {seq_type} sequences for statistical comparison (found {count})")
                insufficient_data = True
        
        if insufficient_data:
            print("\nPlease record more sequences using live_monitor.py before comparing")
            return
            
        # Print comparison header
        comparison_str = " vs ".join(sequence_types)
        print(f"\nAnalyzing characteristics: {comparison_str}")
        print("=" * 80)
        
        # Calculate and print statistics for each sequence type
        for seq_type in sequence_types:
            type_data = filtered_df[filtered_df['label'] == seq_type]
            if not type_data.empty:
                print(f"\n{seq_type.upper()} Sequences (Count: {len(type_data)})")
                print("-" * 40)
                
                metrics = [
                    'motion_magnitude', 'vertical_movement', 'horizontal_movement',
                    'max_velocity', 'mean_velocity', 'vertical_speed_mean',
                    'pattern_coherence_mean', 'height_stability_mean',
                    'width_stability_mean', 'expansion_rate_max', 'mean_coverage'
                ]
                
                for metric in metrics:
                    if metric in type_data.columns:
                        mean_val = type_data[metric].mean()
                        std_val = type_data[metric].std()
                        print(f"{metric:25}: {mean_val:8.3f}  {std_val:6.3f}")
        
        # Create visualizations
        plt.figure(figsize=(15, 10))
        
        # Motion characteristics
        plt.subplot(221)
        sns.boxplot(data=filtered_df, x='label', y='motion_magnitude')
        plt.title('Motion Magnitude by Activity Type')
        
        # Velocity characteristics
        plt.subplot(222)
        sns.boxplot(data=filtered_df, x='label', y='max_velocity')
        plt.title('Maximum Velocity by Activity Type')
        
        # Coverage characteristics
        plt.subplot(223)
        sns.boxplot(data=filtered_df, x='label', y='mean_coverage')
        plt.title('Average Sensor Coverage by Activity Type')
        
        # Movement characteristics
        plt.subplot(224)
        sns.scatterplot(data=filtered_df, x='vertical_movement', 
                      y='horizontal_movement', hue='label', style='label')
        plt.title('Movement Pattern Analysis')
        
        plt.tight_layout()
        plt.show()
    else:
        print("No fall and walk sequences found for comparison")
    
    # Show sequence visualizations if requested
    if args.visualize:
        print("\nPress Ctrl+C to stop the animation and proceed")
        try:
            analyzer.visualize_sequence_comparison()
        except KeyboardInterrupt:
            plt.close()
    
    # Generate detailed report if requested
    if args.report:
        if not analysis_data.empty:
            report = analyzer.generate_analysis_report(analysis_data)
            print("\n" + report)
    
    # Analyze post-fall patterns
    post_fall_patterns = analyzer.analyze_post_fall_patterns()
    print("\nPost-Fall Patterns:")
    print(post_fall_patterns.describe())

    # Generate post-fall report
    post_fall_report = analyzer.generate_post_fall_report(post_fall_patterns)
    print("\n" + post_fall_report)

if __name__ == "__main__":
    main()
